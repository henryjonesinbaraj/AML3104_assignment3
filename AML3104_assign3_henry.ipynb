{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023F-T3 AML 3104 - Neural Networks and Deep Learning 01 (DSMM Group 1 & Group 2)\n",
    "\n",
    "# Assignment_3 - Decision_Tree_Assignment\n",
    "\n",
    " # Name: Henry Jones Inbaraj\n",
    "# Student Id: C0863081"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1. Describe the decision tree classifier algorithm and how it works to make predictions\n",
    "\n",
    "It is a supervised learning method that makes predictions by recursively splitting the data into subsets based on the features, ultimately leading to a decision or class assignment. Decision trees are particularly interpretable and can be visualized, making them valuable for understanding and explaining the decision-making process.\n",
    "\n",
    "Data Preparation:\n",
    "\n",
    "The algorithm begins with a dataset consisting of labeled examples, where each example has a set of features (attributes) and a corresponding class label (the target variable).\n",
    "\n",
    "Tree Construction:\n",
    "\n",
    "The decision tree building process starts at the root node, which represents the entire dataset.\n",
    "The algorithm selects a feature from the dataset to split the data into subsets. It chooses the feature based on a criterion like Gini impurity, entropy, or information gain. These criteria help determine the best feature that provides the most useful separation of data points into different classes.\n",
    "The selected feature is used to create child nodes, and the dataset is divided into subsets based on the values of that feature.\n",
    "This process is repeated recursively for each child node until a stopping condition is met. Stopping conditions can include reaching a maximum depth, having a minimum number of samples in a node, or achieving pure subsets (all examples in a subset belong to the same class).\n",
    "\n",
    "Leaf Node Assignment:\n",
    "\n",
    "Once a stopping condition is met, a leaf node is created, and it is assigned a class label. The majority class of the data points in that leaf node is typically used as the predicted class for all instances that reach this node.\n",
    "\n",
    "Prediction:\n",
    "\n",
    " To make a prediction for a new, unseen instance, you start at the root node and traverse the tree by evaluating the feature values of the instance.\n",
    " At each internal node, you follow the branch that corresponds to the value of the feature in the instance.\n",
    "You continue navigating the tree until you reach a leaf node. The class label assigned to that leaf node is the prediction for the instance.\n",
    "\n",
    "Model Evaluation:\n",
    "\n",
    "Decision tree classifiers can be evaluated using various metrics like accuracy, precision, recall, F1-score, and confusion matrix. You can also assess the model's performance using techniques like cross-validation to ensure it generalizes well to unseen data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n",
    "\n",
    "The fundamental principles involve using information gain, Gini impurity, or entropy to choose the best feature and split points for the nodes of the tree.\n",
    "\n",
    "Initialize the Root Node\n",
    "Calculate Impurity: Gini Impurity  & Entropy\n",
    "Entropy measures the uncertainty or disorder in a set of data points\n",
    "\n",
    "\n",
    "Select the Best Split:\n",
    "\n",
    "To determine how to split the data at a node, you calculate the impurity of potential splits for each feature. The feature that results in the most significant reduction in impurity is chosen as the feature to split on.\n",
    "\n",
    "For each feature, calculate a measure of impurity for each possible split point. For Gini impurity, you calculate the weighted sum of impurities for each split, and for entropy, you calculate the weighted sum of entropies.\n",
    "\n",
    "The feature and split point that result in the lowest impurity are selected.\n",
    "\n",
    "Split the Data:\n",
    "\n",
    "Split the data based on the chosen feature and split point. This creates child nodes for the current node.\n",
    "Repeat for Child Nodes:\n",
    "\n",
    "Repeat the process recursively for each child node, calculating impurity, selecting the best split, and splitting the data further until a stopping criterion is met.\n",
    "Stopping Criterion:\n",
    "\n",
    "A stopping criterion could be reaching a maximum depth, having a minimum number of samples in a node, or achieving pure subsets (all examples in a subset belong to the same class).\n",
    "Assign Class Labels:\n",
    "\n",
    "When you reach a leaf node, you assign a class label to that node based on the majority class of the data points at that node.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n",
    "\n",
    "Start with a labeled dataset that consists of examples, where each example is associated with a set of features (attributes) and one of two possible class labels, often denoted as \"0\" and \"1\" or \"negative\" and \"positive.\"\n",
    "\n",
    "The decision tree building process begins with the entire dataset, representing the root node of the tree.\n",
    "\n",
    "The algorithm selects a feature from the dataset to split the data into two subsets.\n",
    "The selection of the best feature is determined using a criterion like Gini impurity, entropy, or information gain, with the goal of reducing impurity and effectively separating the data into the two classes.\n",
    "The selected feature is used to create child nodes, and the data is divided into two subsets based on the feature values. One subset typically corresponds to one class label, and the other corresponds to the other class label.\n",
    "\n",
    "\n",
    "The process is repeated recursively for each child node. This involves selecting the best feature to split the data at each node and creating additional child nodes.\n",
    "The recursion continues until a stopping condition is met, such as reaching a predefined tree depth or having a minimum number of samples in a nod\n",
    "\n",
    "Once a stopping condition is met, a leaf node is created, and it is assigned a class label. The majority class of the data points in that leaf node is typically used as the predicted class for all instances that reach this node.\n",
    "\n",
    "To make a binary classification prediction for a new, unseen instance, you start at the root node and traverse the tree by evaluating the feature values of the instance.\n",
    "At each internal node, you follow the branch that corresponds to the value of the feature in the instance.\n",
    "You continue navigating the tree until you reach a leaf node. The class label assigned to that leaf node is the prediction for the instance, which will be one of the two binary classes.\n",
    "\n",
    "In short, a decision tree classifier is used for binary classification by recursively splitting the data into two subsets based on features. It assigns class labels to leaf nodes, and predictions are made by traversing the tree from the root to a leaf node. This method is useful for categorizing data into one of two classes and is interpretable and easy to understand.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions\n",
    "\n",
    "The geometric intuition behind decision tree classification involves the idea of recursively partitioning the feature space into regions or \"decision regions.\" These decision regions correspond to different class labels, and the decision tree seeks to draw boundaries that best separate the data points belonging to each class. Here's how the geometric intuition can help understand decision tree classification and its use for making predictions:\n",
    "\n",
    "Feature Space Partitioning:\n",
    "Imagine the feature space as a multi-dimensional space where each feature represents an axis. For binary classification, you have two classes, and the goal is to create decision boundaries that separate these classes in this feature space.\n",
    "The root node of the decision tree represents the entire feature space.\n",
    "\n",
    "Node Splits:\n",
    "At each node in the decision tree, a feature is chosen to split the data. The choice of the feature and the split point (threshold) corresponds to creating a partition along that feature's axis.\n",
    "The split divides the feature space into two regions based on the feature's values.\n",
    "\n",
    "Recursive Partitioning:\n",
    "This splitting process is performed recursively for each child node, creating a tree structure.\n",
    "Each split refines the partitioning, forming smaller and more specific decision regions.\n",
    "\n",
    "Decision Regions:\n",
    "The leaf nodes of the decision tree represent the smallest, most specific decision regions. These regions correspond to class labels, and the majority class in a region becomes the prediction for that region.\n",
    "\n",
    "Prediction:\n",
    "To make a prediction for a new data point, you place it in the feature space, and the decision tree guides you through the tree structure.\n",
    "Starting from the root node, you follow the path by comparing the feature values of the data point with the split criteria at each internal node.\n",
    "You continue navigating the tree until you reach a leaf node, which corresponds to a specific decision region.\n",
    "The class label associated with that leaf node is assigned as the prediction for the data point.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model.\n",
    "\n",
    "A confusion matrix is a table used in the evaluation of the performance of a classification model. It provides a comprehensive summary of the model's predictions and their correspondence to the actual class labels in a binary or multiclass classification problem. The confusion matrix is especially useful for understanding the types of errors a model makes and assessing its performance. It is also known as an error matrix.\n",
    "\n",
    "True Positives (TP): The number of instances correctly predicted as positive (belonging to the positive class).\n",
    "\n",
    "True Negatives (TN): The number of instances correctly predicted as negative (belonging to the negative class).\n",
    "\n",
    "False Positives (FP): The number of instances incorrectly predicted as positive when they are actually negative. These are also known as Type I errors or false alarms.\n",
    "\n",
    "False Negatives (FN): The number of instances incorrectly predicted as negative when they are actually positive. These are also known as Type II errors or misses.\n",
    "\n",
    "\n",
    "Precision measures the proportion of correct positive predictions made by the model. It tells you, out of the instances predicted as positive, how many were actually correct\n",
    "\n",
    "Recall, also known as sensitivity or true positive rate (TPR), tells you the proportion of actual positive instances that the model managed to correctly identify. \n",
    "\n",
    "F1 score is a single metric that balances precision and recall. It's essentially a way to assess the overall accuracy and reliability of your model. The F1 score is the harmonic mean of precision and recall. It ranges from 0 (when either precision or recall is very low) to 1 (when both precision and recall are high). It's a useful measure for gauging the overall quality of your model's performance, taking into account both its ability to make accurate positive predictions and its effectiveness in capturing actual positive instances.\n",
    "\n",
    "Accuracy is a proportion of predictions that the model classified correctly.\n",
    "\n",
    "FPR= (FP+TN)/FP, \n",
    "Specificity= (TN+FP)/ TN, \n",
    "F1−Score=  (2⋅(Precision⋅Recall))/Precision+Recall, \n",
    "Recall= (TP+FN)/TP,\n",
    "Precision= (TP+FP)/TP, Accuracy= (TP+TN+FP+FN)/ (TP+TN)\n",
    "\n",
    "By examining these metrics and the confusion matrix, you can gain a deeper understanding of a classification model's performance, make informed decisions about model selection or parameter tuning, and identify areas for improvement. The choice of evaluation metric depends on the specific goals and requirements of the classification problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it\n",
    "\n",
    "Suppose you have a binary classification model for a medical test, where:\n",
    "\n",
    "True Positives (TP) = 80\n",
    "True Negatives (TN) = 120\n",
    "False Positives (FP) = 20\n",
    "False Negatives (FN) = 10\n",
    "Now, you can calculate precision, recall, and the F1 score:\n",
    "\n",
    "Precision = 80 / (80 + 20) = 0.80 (or 80%)\n",
    "Recall = 80 / (80 + 10) = 0.89 (or 89%)\n",
    "F1 Score = 2 * (0.80 * 0.89) / (0.80 + 0.89) ≈ 0.845 (or 84.5%)\n",
    "These metrics provide a quantitative assessment of your classification model's performance, with precision indicating the accuracy of positive predictions, recall indicating the ability to capture actual positive instances, and the F1 score offering a balanced measure of both precision and recall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done.\n",
    "\n",
    "Choosing an appropriate evaluation metric for a classification problem is crucial because it directly influences how you assess the performance of your model and make decisions about its effectiveness.\n",
    "\n",
    "Alignment with Problem Objectives: The goals of various classification problems vary. For example, you might be more concerned with reducing false positives (regular emails flagged as spam) in email spam detection than with minimising false negatives (missed cases) in medical diagnostics. The metric selected should be in line with these particular objectives.\n",
    "\n",
    "Managing Class Imbalance: In a lot of real-world situations, there can be an imbalance in the number of instances of one class compared to the other. In these situations, accuracy on its own may be deceptive. Specific metrics that measure a model's performance more precisely include precision, recall, and F1-score.\n",
    "\n",
    "Cost Sensitivity: Some misclassifications can be costlier than others. For example, in fraud detection, a false positive (flagging a non-fraudulent transaction as fraud) can inconvenience customers, while a false negative (missing a fraudulent transaction) can be very costly. The choice of metric can reflect the cost sensitivity of the problem.\n",
    "\n",
    "Gain a thorough understanding of the classification problem and its practical implications by reading through the problem and business context. Take into account the particular objectives, limitations, and expenses related to the issue. You can determine which evaluation metrics are most pertinent by using this context.\n",
    "\n",
    "Choose Measures Based on Objectives:\n",
    "\n",
    "Accuracy: Often used as a starting point, but potentially unsuitable in cases of imbalanced datasets or when the outcomes of false positives and negatives differ greatly.\n",
    "precision: When reducing false positives is important, use precision (e.g., spam detection, medical diagnoses).\n",
    "Recall: Use in situations where reducing false negative results is essential (fraud detection, disease detection, etc.).\n",
    "F1-Score: In situations where both false positives and false negatives are significant, the F1-Score strikes a balance between precision and recall.\n",
    "Specificity: Helpful in situations where false positives raise more red flags, like airport security.\n",
    "\n",
    "Use Domain Knowledge: Consult domain experts and stakeholders who have a deep understanding of the problem. They can provide valuable insights into which metrics are most relevant and why.\n",
    "\n",
    "Experiment with Different Metrics: Consider evaluating your model using multiple metrics to get a more comprehensive view of its performance. This can help in understanding trade-offs and selecting the most appropriate metric for your problem.\n",
    "\n",
    "Threshold Adjustment: In some cases, you can adjust the prediction threshold of your model to achieve a desired trade-off between precision and recall. This is especially relevant when dealing with imbalanced datasets.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Provide an example of a classification problem where precision is the most important metric, and explain why.\n",
    "\n",
    "\n",
    "Precision is a crucial metric in classification problems, like those found in medical diagnostic tests for serious or life-threatening illnesses like cancer. The goal in this case is to reduce the number of false positives, or the instances in which a patient is diagnosed with an illness when they do not.\n",
    "\n",
    "Here's why, in this instance, precision is the most crucial metric:\n",
    "\n",
    "Reducing False Positives: In the medical field, false positives occur when a patient is incorrectly given a serious diagnosis when they are, in reality, healthy. This may result in needless worry, anxiety, and medical procedures, such as invasive and possibly dangerous therapies. It may also put a burden on medical resources.\n",
    "\n",
    "Patient Well-Being: Patients may experience severe psychological and physical effects from false positive diagnoses. It may result in further needless medical procedures and costs, as well as emotional distress. Optimising accuracy lowers the likelihood of these unfavourable outcomes.\n",
    "\n",
    "Ethical Considerations: These are very important in the medical field. The cornerstone of healthcare is the \"do no harm\" principle. A high-precision model lowers the possibility of misdiagnosing patients and endangering them.\n",
    "\n",
    "Healthcare Costs: Patients, insurers, and healthcare providers may all have to pay more for unnecessary medical procedures, treatments, and surgeries. Precision reduces needless procedures, which helps with cost control.\n",
    "\n",
    "Compliance and Trust: Patients and healthcare providers alike benefit from high precision. Physicians are more likely to rely on a diagnostic tool with a low false positive rate for clinical decision-making, and patients are more likely to trust it.\n",
    "\n",
    "Because false positive errors have serious consequences, precision is preferred over other metrics like recall or F1 score in medical diagnostic contexts. Achieving a balance between precision and recall is frequently the main objective to guarantee that the risk of causing harm through incorrect diagnoses is minimised, even though it's critical to reduce false negatives (missed diagnoses).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Provide an example of a classification problem where recall is the most important metric and explain why\n",
    "\n",
    "Recall is an important metric to focus on when it comes to airport security screening. Finding illegal or dangerous goods (such as weapons or explosives) in passengers' carry-on luggage is the aim of airport security. Recall is crucial in this situation for the following reasons:\n",
    "\n",
    "Reducing False Negatives: When a potentially harmful item is overlooked or undetected during the security screening procedure, this is known as a false negative. Making recall a priority entails making sure that a significant portion of real threats are appropriately recognised. Ignoring a potentially hazardous item could have serious repercussions, such as endangering passengers and compromising aviation security.\n",
    "\n",
    "Safety and Security: Ensuring the safety of travellers and aircraft is the main objective of airport security. There are serious security risks when a dangerous item is missed, including the potential for terrorist attacks or hijackings. Recall optimisation contributes to increased aviation security and safety.\n",
    "\n",
    "Public Trust: Passengers anticipate a high standard of security and safety at airports. The public may become less confident in the efficacy of airport security procedures if incidents occur where hazardous materials escape detection. A recall-focused approach upholds public confidence and shows a dedication to passenger safety.\n",
    "\n",
    "Regulatory Compliance: Tight guidelines and supervision are in place for airport security. In order to guarantee the effectiveness and resilience of security screening processes, regulatory bodies frequently demand compliance with recall targets.\n",
    "\n",
    "Legal and Reputational Repercussions: Airport security personnel and agencies may face serious legal and reputational repercussions if a dangerous item is overlooked. The focus on recall guarantees a proactive approach to security and lowers the possibility of such outcomes.\n",
    "\n",
    "Recall is critical to airport security screening because it prioritises finding dangerous items, supports aviation security as its primary objective, builds public trust, assures regulatory compliance, and reduces legal and reputational risks. Prioritising recall is crucial in this high-stakes classification problem to protect travellers and maintain the integrity of air travel.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://datascience.stackexchange.com/questions/10228/when-should-i-use-gini-impurity-as-opposed-to-information-gain-entropy#:~:text=Below%20are%20the%20formulae%20of,pjlogpj\n",
    "\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/#:~:text=A%20Confusion%20matrix%20is%20an,by%20the%20machine%20learning%20model.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
